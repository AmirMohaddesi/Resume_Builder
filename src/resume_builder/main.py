#!/usr/bin/env python
"""
Resume Builder main entry point with streamlined UI.

This is a deterministic + CrewAI resume generation pipeline.
Deterministic Python functions handle validation and orchestration; CrewAI agents handle content generation.

Usage:
    # Run UI
    python -m resume_builder.main
    # or
    crewai run

    # Run with debug mode (produces pipeline_status_debug.json)
    # Set debug=True in run_pipeline() or enable in UI checkbox

Debug Mode:
    When debug=True, the orchestrator produces pipeline_status_debug.json with:
    - Step-by-step orchestration trace
    - Detected input files
    - Phase state explanations
    - Reasoning for ready_for_latex decision
"""
from __future__ import annotations

import os
import sys
from pathlib import Path
from datetime import datetime
from typing import Optional, Tuple, Dict, Any
import traceback

# Load .env once
try:
    from dotenv import load_dotenv  # type: ignore
    load_dotenv()
except Exception:
    pass

# Import ResumeTeam; allow running as package or script
try:
    from resume_builder.crew import ResumeTeam
    from resume_builder.logger import init_logger, get_logger
    from resume_builder.orchestration import run_pipeline, run_template_matching
    from resume_builder.ui import build_ui, run_ui
except Exception:
    sys.path.insert(0, str(Path(__file__).resolve().parent))
    from crew import ResumeTeam  # type: ignore
    from logger import init_logger, get_logger  # type: ignore
    from orchestration import run_pipeline, run_template_matching  # type: ignore
    from ui import build_ui, run_ui  # type: ignore

# ---------- Paths & defaults ----------
from resume_builder.paths import (
    PROJECT_ROOT, OUTPUT_DIR, GENERATED_DIR, LOG_DIR, TEMPLATES, ensure_dirs
)

# Initialize logging
_logger = init_logger(LOG_DIR, log_level=os.getenv("LOG_LEVEL", "INFO"))

DEFAULT_TEMPLATE_PATH = Path(
    os.getenv("TEMPLATE_PATH", TEMPLATES / "main.tex")
)
DEFAULT_PROFILE_PATH = Path(
    os.getenv("PROFILE_PATH", PROJECT_ROOT / "src" / "resume_builder" / "data" / "profile.json")
)

FINAL_PDF = GENERATED_DIR / "final_resume.pdf"
RENDERED_TEX = GENERATED_DIR / "rendered_resume.tex"
PDF_REVIEW_JSON = OUTPUT_DIR / "pdf_review_report.json"
COVER_LETTER_PDF = GENERATED_DIR / "cover_letter.pdf"
COVER_LETTER_TEX = GENERATED_DIR / "cover_letter.tex"


def _ensure_output_dir() -> None:
    """Ensure output directories exist."""
    ensure_dirs()


def _clean_generated_files() -> None:
    """Clean up old generated files before a new run."""
    if GENERATED_DIR.exists():
        for file in GENERATED_DIR.glob("*.tex"):
            try:
                file.unlink()
                _logger.debug(f"Removed old file: {file}")
            except Exception as e:
                _logger.warning(f"Could not remove {file}: {e}")
        for file in GENERATED_DIR.glob("*.pdf"):
            try:
                file.unlink()
                _logger.debug(f"Removed old PDF: {file}")
            except Exception as e:
                _logger.warning(f"Could not remove {file}: {e}")


def _clean_json_files() -> None:
    """Clean up JSON files from previous runs to ensure fresh data.
    
    This prevents contamination between runs by removing all generated JSON files.
    NOTE: user_profile.json is NOT cleaned because it can be an input file
    (saved from previous runs) that agents need to read. It will be overwritten
    by agents if they generate a new one.
    """
    # Comprehensive list of all JSON files generated by the pipeline
    json_files_to_clean = [
        # Profile and validation files
        "validated_profile.json",
        "profile_llm_view.json",
        "file_collection_report.json",
        "preflight_check.json",
        
        # Job description files
        "parsed_jd.json",
        "job_description.json",
        
        # Content selection files
        "selected_experiences.json",
        "selected_projects.json",
        "selected_skills.json",
        
        # Content writing files
        "summary.json",
        "education.json",
        "header.json",
        
        # Quality and analysis files
        "ats_report.json",
        "ats_rules_report.json",
        "privacy_report.json",
        
        # Template and pipeline files
        "template_validation.json",
        "tailor_plan.json",
        "pipeline_status.json",
        "pipeline_status_debug.json",
        
        # Cover letter files
        "cover_letter.json",
        
        # Debug and adjustment files
        "crew_execution_debug.json",
        "adjustment_request.json",
        "error_log.json",
    ]
    
    # Also clean any JSON files in output/ that match common patterns
    # This catches any files we might have missed
    if OUTPUT_DIR.exists():
        for json_file in OUTPUT_DIR.glob("*.json"):
            # Skip user_profile.json as it can be an input file
            if json_file.name == "user_profile.json":
                continue
            # Skip progress.json as it's actively updated during execution
            if json_file.name == "progress.json":
                continue
            # If file is not in our explicit list, still clean it if it's a generated file
            if json_file.name not in json_files_to_clean:
                # Only clean files that look like generated files (not user input)
                if any(pattern in json_file.name for pattern in [
                    "_block.json", "_report.json", "_status.json", 
                    "selected_", "parsed_", "validated_", "debug.json"
                ]):
                    try:
                        json_file.unlink()
                        _logger.debug(f"Removed generated JSON file: {json_file.name}")
                    except Exception as e:
                        _logger.warning(f"Could not remove {json_file}: {e}")
    
    # Clean explicitly listed files
    for json_file in json_files_to_clean:
        file_path = OUTPUT_DIR / json_file
        if file_path.exists():
            try:
                file_path.unlink()
                _logger.debug(f"Removed old JSON file: {file_path}")
            except Exception as e:
                _logger.warning(f"Could not remove {file_path}: {e}")


# run_pipeline and run_template_matching are now in orchestration.py
# Imported at the top of this file
# build_ui and run_ui are now in ui.py
# Imported at the top of this file


# ---------- CLI Entrypoints ----------
def run() -> None:
    """CrewAI CLI entrypoint."""
    jd_path = PROJECT_ROOT / "job_description.txt"
    if not jd_path.exists():
        example = PROJECT_ROOT / "job_description.example.txt"
        jd_text = example.read_text(encoding="utf-8") if example.exists() else ""
    else:
        jd_text = jd_path.read_text(encoding="utf-8")

    pdf_path, msg, _ = run_pipeline(jd_text=jd_text, profile_path=None)
    print(msg)
    if pdf_path:
        print(f"PDF: {pdf_path}")


def run_crew() -> int:
    """Console entry used by `crewai run`."""
    run_ui()
    return 0


if __name__ == "__main__":
    run_ui()

